<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Telematics Combined Actuarial Neural Networks for claim count data</title>
    <meta charset="utf-8" />
    <meta name="author" content="Francis Duval" />
    <meta name="date" content="2023-05-12" />
    <script src="libs/header-attrs-2.21/header-attrs.js"></script>
    <script src="libs/fabric-4.3.1/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble-0.0.1/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble-0.0.1/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30,"palette":[]}) })</script>
    <link rel="stylesheet" href="theme_cara.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide
background-image: url(images/logo_chaire.jpg), url(images/background.jpg)
background-size: 30%, cover
background-position: 98% 98%, center




.titre-page-titre[Telematics Combined Actuarial Neural Networks for Claim Count Data]
&lt;br /&gt;
.sous-titre-page-titre[Co-operators Show and Share]
&lt;br /&gt;
&lt;br /&gt;
***
&lt;br /&gt;
&lt;br /&gt;
.sous-sous-titre-page-titre[.mon-style-bleu[par] Francis Duval (Jean-Philippe Boucher, Mathieu Pigeon) &lt;br&gt; .mon-style-bleu[à] l'Université du Québec à Montréal &lt;br&gt; .mon-style-bleu[le] 17 mai 2023]

???

- Francis Duval, PhD candidate member of the CARA Chair.
- Research project that is part of my thesis.
- Many thanks to Marc Morin from the BI team for helping me with the coding of the neural networks in Torch and for helping me developing my intuition in deep learning.

---

# Introduction

.left-column[
&lt;img src="images/tele_icon.png" width="60%" style="display: block; margin: auto;" /&gt;

&lt;img src="images/brain_icon.png" width="65%" style="display: block; margin: auto;" /&gt;

&lt;img src="images/nn_icon.png" width="60%" style="display: block; margin: auto;" /&gt;
]

.right-column[
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.ecriture-grise[For several years now, insurers have been using telematics information in their pricing models.]
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.ecriture-grise[Most of the time, human brainpower is used to extract features from raw data that are thought to be correlated with claiming risk.]
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.ecriture-grise[What if we automated this feature engineering process with a neural network?]
]

???

- Insurers have been leveraging telematics data in their pricing models for several years now.
- Using telematics data in pricing model has many benefits:
  - More precise pure premium
  - Incentive for safer and less frequent driving (less accidents, congestion and pollution)
  - Substitute to sensible rating factors
  - Etc.
- Insurers will most of the time take the raw telematics data and try to extract features from it that they think are correlated with the claiming risk:
  - harsh acceleration/braking
  - % night driving
  - % driving in different speed buckets
  - Etc.
- Good ad hoc approach, but might be suboptimal
- Since we know that deep neural networks are good at learning useful features from raw data, why not feed the telematics data to a multilayer perceptron and let it extract the features itself?

---

# Introduction
&lt;div class="neg-break"&gt;&lt;/div&gt;
## Combined Actuarial Neural Network (CANN) approach
&lt;div class="neg-break"&gt;&lt;/div&gt;
.pull-left[
&lt;img src="images/CANN.png" width="90%" style="display: block; margin: auto;" /&gt;

.center[Figure taken from [this paper](https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID3320525_code769240.pdf?abstractid=3320525)
]].
&lt;/br&gt;
&lt;/br&gt;
&lt;/br&gt;
.ecriture-grise[Consists of a .bleu-gras[classical parametric model] (often a GLM) to which a .bleu-gras[neural network] has been attached.]

.ecriture-grise[The goal of the neural network part is to .bleu-gras[capture] any .bleu-gras[signal] that might have been missed by the GLM.]

.ecriture-grise[Parameters for the GLM part are typically initialized at .bleu-gras[maximum likelihood], while those of the neural network part are initialized to .bleu-gras[zero].]

???

- Here is the generic neural network (NN) architecture we use to model the number of claims.
- Developed by Mario V. Wüthrich And Michael Merz, it is called the "Combined Actuarial Neural Network", or CANN for short.
- It's a NN that consists of 2 parts: the GLM part and the network part.
- The GLM part acts like a regular GLM, while the network attached to it aims at capturing the signal that have been missed by the GLM.
- Indeed, a GLM is not able to capture interactions between predictors and can only approximate linear functions of predictors. The network part helps to improve performance.
- One will usually initialize the parameters of the GLM part at maximum likelihood, while the parameters of the network part will be initialized at zero.
- That way, the whole NN already gives decent predictions at initialization. When the training starts, the network part is in fact trained on the residuals of the GLM, and we can thus see the procedure as a NN boosting of the GLM.
- This CANN architecture is good because:
  - The GLM part allows to have more interpretability
  - The CANN is initialized at decent GLM predictions, which makes the training faster.
  - It is very flexible. In this example, both parts are fed with the same inputs. However, one could include other types of inputs in the network part. In general, we know that NN are good at dealing with unstructured data such as telematics data and text data.
  - The fact that the network part is attached to the GLM part ensures that the network does not catch the signal that has already been caught by the GLM.
- It would even be possible, with this architecture, to extract features from the hidden layers and then use them in a regular GLM.
- In brief, this architecture could be useful in any supervised learning problem, particularly when unstructured data is available.

---

background-image: url(images/perceptron.png)
background-size: 75%
background-position: 100% 70%

# GLM as a neural network

.ecriture-grise[Equivalence table]
.left-column[
| Perceptron | GLM |
|--------|-----------|
| Bias `\(w_0\)` | `\(\beta_0\)` |
| Weights `\(w_1, \dots, w_m\)` | `\(\beta_1, \dots, \beta_p\)` |
| Activation function `\(g\)` | Inverse link function `\(g^{-1}\)` |
]

???

- You may wonder how it is possible to embed a GLM with a neural network.
- In fact, this is easy because the prediction function of a GLM is the same as a perceptron, which is the building block of neural networks.
- So in both in a GLM and a perceptron, the inputs are each multiplied by a weight. the products are then added together before being passed to an activation function. Here the activation function is the sigmoid, so we have a Bernoulli GLM, or logistic regression.
- Since we want to model claim count, our activation function will be the sofplus since we want positive outputs.

---

# Data

## Traditional risk factors 
&lt;div style="margin-top: -70px; margin-left: 50px; margin-bottom: 20px;" class="layer"&gt;
  &lt;div class="neuron gris"&gt;&lt;/div&gt;
  &lt;div class="neuron gris"&gt;&lt;/div&gt;
  &lt;div class="neuron gris"&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;!-- &lt;div style="margin-top: -60px; margin-left: 450px; margin-bottom: 40px;" class="neuron gris"&gt;&lt;/div&gt; --&gt;


| Name                | Description                                              | Type        |
|---------------------|----------------------------------------------------------|-------------|
| `annual_distance`   | Annual distance declared by the insured                  | Numeric     |
| `commute_distance`  | Distance to the place of work declared by the insured    | Numeric     |
| `conv_count_3_yrs_minor` | Number of minor contraventions in the last 3 years  | Numeric     |
| `gender`            | Gender of the insured                                    | Categorical |
| `marital_status`    | Marital status of the insured                            | Categorical |
| `pmt_plan`          | Payment plan chosen by the insured                       | Categorical |
| `veh_age`           | Vehicle age                                              | Numeric     |
| `veh_use`           | Use of the vehicle                                       | Categorical |
| `years_claim_free`  | Number of years since last claim                         | Numeric     |
| `years_licensed`    | Number of years since obtaining driver's license         | Numeric     |
| `distance`          | Real distance driven                                     | Numeric     |

???

- Here are the traditional risk factors that we use. Note that we include the real distance driven since it is a telematics feature known to have a lot of predictive power.
- In the computational graphs that I will show later, these rating factors are represented by grey circles.

---

# Data

## Extract from the telematics dataset

| Contract ID | Trip ID | Departure datetime | Arrival datetime | Distance | Maximum speed |
|:-----------:|:------:|:-----------------:|:----------------:|:--------:|:-------------:|
| A | 1 | 2017-05-02 19:04:15 | 2017-05-02 19:24:24 | 25.0 | 104 |
| A | 2 | 2017-05-02 21:31:29 | 2017-05-02 21:31:29 | 6.4 | 66 |
| `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` |
| A | 2320 | 2018-04-30 21:17:22 | 2018-04-30 21:18:44 | 0.2 | 27 |
| B | 1 | 2017-03-26 11:46:07 | 2017-03-26 11:53:29 | 1.5 | 76 |
| B | 2 | 2017-03-26 15:18:23 | 2017-03-26 15:51:46 | 35.1 | 119 |
| `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` |
| B | 1485 | 2018-03-23 20:07:08 | 2018-03-23 20:20:30 | 10.1 | 92 |
| C | 1 | 2017-11-20 08:14:34 | 2017-11-20 08:40:21 | 9.7 | 78 |
| `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` |

???

- This is the telematics dataset I have a my disposal. Each row is a trip, which is depicted by 4 attributes: the departure and arrival datetime, the distance driven and the maximum speed reached.
- For instance, contract A has 2320 trips.
- Even though neural networks are known to perform well on raw data, this dataset still needs a minimal amount of preprocessing in order to make it more informative to the NN.

---

# Data

&lt;div style="margin-top: -30px;"&gt;&lt;/div&gt;

## Telematics inputs

&lt;div style="margin-top: -70px; margin-left: -370px; margin-bottom: 20px;" class="layer"&gt;
  &lt;div class="neuron vert"&gt;&lt;/div&gt;
  &lt;div class="neuron vert"&gt;&lt;/div&gt;
  &lt;div class="neuron vert"&gt;&lt;/div&gt;
&lt;/div&gt;

.ecriture-grise[We want the neural network to learn features from the telematics dataset, but we still need a minimal preprocesssing.]

&lt;div style="margin-top: -5px;"&gt;&lt;/div&gt;
.ecriture-grise[We thus define the following vectors:]

.bloc.bleu[
`\(\boldsymbol{h} = (h_1, h_2, \dots, h_{24})\)` where `\(h_i\)` is the fraction of driving in the `\(i^\text{th}\)` hour of the day.

`\(\boldsymbol{p} = (p_1, p_2, \dots, p_7)\)` where `\(p_i\)` is the fraction of driving in the `\(i^\text{th}\)` day of the week.

`\(\boldsymbol{vmo} = (vmo_1, vmo_2, \dots, vmo_{14})\)` where `\(vmo_i\)` is the fraction of trips in the `\(i^\text{th}\)` interval of average speed.

`\(\boldsymbol{vma} = (vma_1, vma_2, \dots, vma_{16})\)` where `\(vma_i\)` is the fraction of trips in the `\(i^\text{th}\)` interval of maximum speed.
]

.ecriture-grise[We then concatenate these 4 vectors into a big input vector of dimension 24 + 7 + 14 + 16 = 61, which will be given as input to the MLP part of the CANN model:]

`\(\textbf{telematics_input_vec} = (\boldsymbol{h}, \boldsymbol{p}, \boldsymbol{vmo}, \boldsymbol{vma})\)`

???

- From this telematics dataset, we therefore create 4 telematics vectors.
- The first one, h, is of dimension 24 (for the 24 hours of the day), and each elements is the fraction of driving in the correponding hour of the day. For instance, `\(h_1\)` is the fraction of driving for a given contract made between midnight and 1:00.
- The second one is similar, but instead records the fraction of driving in each day of the week.
- vmo records the fraction of trips made in different buckets of average speed. I made 10 kilometers per hour buckets, so for instance, vmo_1 is the fraction of trips made at an average speed between 0 and 10 kph. 
- vma is the same, but for maximum speed. 
- We then concatenate these 4 vectors into 1 big telematics vector, which summarises the driving habits of a given contract.
- These telematics inputs are represented by green circles.

---
`$$\newcommand{\Xcal}{\mathcal{X}}$$`
# Poisson regression

&lt;div style="margin-top: -60px;"&gt;&lt;/div&gt;
**Assumptions:**
- Given its predictors `\(\boldsymbol{x}_i\)`, the PMF of `\(Y_i\)`, the number of claims for contract `\(i\)`, is given by:
`\begin{align}
P(Y_i = y_i|\boldsymbol{X}_i) = \frac{e^{-\mu(\boldsymbol{x_i})}\mu(\boldsymbol{x_i})^{y_i}}{y_i!}, \quad y_i = 0, 1, \dots,
\end{align}`
with
`\begin{align}
\mathbb{E}(Y_i) = \text{Var}(Y_i) = \mu(\boldsymbol{x_i})
\end{align}`

**Goal:**
- Find a good regression function `\(\mu: \Xcal \rightarrow \mathbb{R}^+\)` mapping the predictors `\(\boldsymbol{x}\)` to the parameter `\(\mu\)`.

**How to proceed:**
- Choose a specification for the regression function `\(\mu(\cdot)\)` 
- Minimise the negative log-likelihood over the training set:
`\begin{align}
\underset{\mu}{\text{minimize}} \left\{ -\frac{1}{n} \sum_{i \in \mathcal{T}} y_i \ln\left[\mu(\boldsymbol{x_i})\right] - \mu(\boldsymbol{x}_i)\right\}
\end{align}`

???

- First specification we consider is Poisson, which means the number of claims is assumed to follow a Poisson distribution.
- The goal is the find a good regression function mapping the predictors x to the mean parameter mu.
- A way to proceed is to first choose a specification for the regression function space: it can be a GLM, a neural network, a random forest, etc.
- Then, we can search the function space and try to minimize the negative log-likelihood. With NN, this minimization procedure is carried on with gradient descent.

---

background-image: url(images/PoissonCANN.png)
background-size: 60%
background-position: 80% 80%

# Poisson CANN

.pull-left[
&lt;img src="images/PoissonCANN_legend.png" width="60%" style="display: block; margin: auto auto auto 0;" /&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
- The output `\(\mu\)` is compared with the response `\(y\)` to compute the negative log-likelihood.
]

???

- This is the CANN architecture for the Poisson specification.
- The GLM part has only the traditional risk factors as inputs, while the network part, which is a MLP, has both traditional risk factors and the telematics vector as inputs. This allows to consider traditional-telematics interactions.
- Note that the MLP part has in fact 3 hidden layers, and not 2 as shown.
- So the output of the GLM part is added to the output of the MLP part, and then this sum is passed through the softplus function to ensure a positive mu output.
- The loss function used is the Poisson negative log-likelihood.
- I will not go through the details of the MLP part, but you can simply think of it as a complex function approximation machine that complements the GLM.

---

`$$\newcommand{\Xcal}{\mathcal{X}}$$`
# Negative binomial regression

**Assumptions:**
- Given its predictors `\(\boldsymbol{x}_i\)`, the PMF of `\(Y_i\)`, the number of claims for contract `\(i\)`, is given by:
`\begin{align}
P(Y_i = y_i) = \frac{\Gamma(y_i + \phi)}{y_i! \Gamma(\phi)} \left(\frac{\phi}{\phi + \mu(\boldsymbol{x}_i)}\right)^\phi \left( \frac{\mu(\boldsymbol{x}_i)}{\mu(\boldsymbol{x}_i) + \phi}\right)^{y_i}, \quad y_i = 0, 1, \dots,
\end{align}`
with
`\begin{align}
\mathbb{E}(Y_i) = \mu(\boldsymbol{x}_i) \quad \text{and} \quad \text{Var}(Y_i) = \mu(\boldsymbol{x}_i) + \frac{\mu(\boldsymbol{x}_i)^2}{\phi}
\end{align}`

**How to proceed**:

- We seek for a good regression function `\(\mu: \Xcal \rightarrow \mathbb{R}^+\)` mapping the predictors `\(\boldsymbol{x}\)` to the parameter `\(\mu\)` and a good estimate of `\(\phi\)`.
- Again, we proceed by minimising the negative log-likelihood.
- With this specification, we will need a 2-output neural network

???

- The second specification we consider is the negative binomial.
- The number of claim is now assumed to follow a NB distribution, which has a parameter allowing for overdipersion, which is often the case with claim count data.
- Since the distribution has 2 parameters, we will need a NN with 2 outputs.

---

background-image: url(images/NB2CANN.png)
background-size: 60%
background-position: 80% 80%

# Negative binomial CANN

.pull-left[
&lt;img src="images/PoissonCANN_legend.png" width="60%" style="display: block; margin: auto auto auto 0;" /&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
- The outputs `\(\mu\)` and `\(\phi\)` are compared with the response `\(y\)` to compute the negative log-likelihood.
]

???

- This is the CANN architecture for the NB specification.
- The only thing that changes is the loss function and the addition of an output neuron to accomodate the new parameter phi.
- Note that we did not include heterogeneity in the phi parameter. 

---

# Multivariate Negative Binomial (MVNB) Regression

&lt;!-- .ecriture-grise[Suited for longitudinal count data]  --&gt;

**Assumptions:**

- The claim vector for insured `\(i\)`, `\((Y_{i1}, \dots, Y_{iT_i})\)`, follows a MVNB distribution:

`\begin{align}
  (Y_{i1}, \dots, Y_{iT_i}) \sim MVNB
\end{align}`

&lt;!-- \begin{align} --&gt;
&lt;!-- P(Y_{i1} = y_{i1}, \dots, Y_{i,T_i} = y_{i,T_i}) = \left(\prod_{t=1}^{T_i} \frac{\mu(\boldsymbol{x}_{it})^{y_{it}}}{y_{it}!}\right) \frac{\Gamma(y_{i\bullet} + \nu)}{\Gamma(\nu)} \left(\frac{\nu}{\sum_{t=1}^{T_i} \mu(\boldsymbol{x}_{it}) + \nu}\right)^\nu \left(\sum_{t=1}^{T_i} \mu(\boldsymbol{x}_{it}) + \nu\right)^{-y_{i\bullet}}, --&gt;
&lt;!-- \end{align} --&gt;
&lt;!-- where `\(y_{i\bullet} = \sum_{t=1}^{T_i}y_{it}\)`. --&gt;

- Hard to train a neural network with a multivariate distribution, but fortunately, we can show that
`\begin{align}
P(Y_{i1} = y_{i1}|\boldsymbol{X}_{i1}) = \frac{\Gamma(y_{i1} + \phi)}{y_{i1}! \Gamma(\phi)} \left(\frac{\phi}{\phi + \mu(\boldsymbol{x}_{i1})}\right)^\phi \left( \frac{\mu(\boldsymbol{x}_{i1})}{\mu(\boldsymbol{x}_{i1}) + \phi}\right)^{y_{i1}}
\end{align}`
and that
`\begin{align}
P(Y_{it} = y_{it}|\boldsymbol{Y}_{i, t-1}, \boldsymbol{X}_{i1}, \dots, \boldsymbol{X}_{i, t-1}) = \frac{\Gamma(y_{it} + \alpha)}{y_{it}! \Gamma(\alpha)} \left(\frac{\gamma}{\gamma + \mu(\boldsymbol{x}_{it})}\right)^\alpha \left( \frac{\mu(\boldsymbol{x}_{it})}{\mu(\boldsymbol{x}_{it}) + \gamma}\right)^{y_{it}},\quad t = 2, 3, \dots, T_i.
\end{align}`
where `\(\alpha = \phi + \sum_{t' = 1}^t y_{it'}\)` and `\(\gamma = \phi + \sum_{t' = 1}^t \mu(\boldsymbol{x}_{it'})\)`.

.ecriture-grise[In essence, modeling longitudinal claim counts with a MVNB is equivalent to modeling with a NB using past history.]

???

- With the Poisson and NB specifications, we assume that the contracts are independent, which is not true.
- Indeed, each insured is observed over many contracts, and the contracts of insured are not independent.
- It is therefore better to have a longitudinal model, and one of the most popular specifications for this is the MVNB.
- So the assumption is that the claim vector for an insured follows a MVNB distribution.
- It is not straightforward to train a NN with a multivariate specification, but fortunately, we can break that down to a univariate problem.
- Indeed, we can show that with the MVNB specification, the number of claims at time t follows a NB distribution with parameters depending on the insured's history.
- This is really the heart of the project because to my knowledge, there is no such model in the actuarial literature.
- Unfortunately, I did not yet code this model, but I'm about to do it. It is not as straightforward to code in Torch as the 2 other specifications because I need to take care of updating the distribution parameters (which depend on past contracts) at each iteration.

---

# Results

.center[
**Poisson specification**

| Model | Mean Squared error | Logarithmic score | 
|:-------------------------------------|:------:|:-----------------:|
| Naïve model (no heterogeneity) | 0.0674 | 0.2441 |
| GLM (only &lt;div class="hover-container"&gt; traditional risk factors &lt;ul class="hover-list"&gt;&lt;li&gt;expo&lt;/li&gt;&lt;li&gt;annual_distance&lt;/li&gt;&lt;li&gt;commute_distance&lt;/li&gt;&lt;li&gt;conv_count_3_yrs_minor&lt;/li&gt;&lt;li&gt;gender&lt;/li&gt;&lt;li&gt;marital_status&lt;/li&gt;&lt;li&gt;pmt_plan&lt;/li&gt;&lt;li&gt;veh_age&lt;/li&gt;&lt;li&gt;veh_use&lt;/li&gt;&lt;li&gt;years_claim_free&lt;/li&gt;&lt;li&gt;years_licensed&lt;/li&gt;&lt;li&gt;distance&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;) | 1.47 % | 3.21 % |
| GLM (traditional risk factors + &lt;div class="hover-container"&gt; handcrafted telematics features &lt;ul class="hover-list"&gt;&lt;li&gt;avg_daily_nb_trips&lt;/li&gt;&lt;li&gt;med_trip_avg_speed&lt;/li&gt;&lt;li&gt;med_trip_distance&lt;/li&gt;&lt;li&gt;med_trip_max_speed&lt;/li&gt;&lt;li&gt;max_trip_max_speed&lt;/li&gt;&lt;li&gt;prop_long_trip&lt;/li&gt;&lt;li&gt;frac_expo_night&lt;/li&gt;&lt;li&gt;frac_expo_noon&lt;/li&gt;&lt;li&gt;frac_expo_evening&lt;/li&gt;&lt;li&gt;frac_expo_peak_morning&lt;/li&gt;&lt;li&gt;frac_expo_peak_evening&lt;/li&gt;&lt;li&gt;frac_expo_mon_to_thu&lt;/li&gt;&lt;li&gt;frac_expo_fri_sat&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;) | 1.72 % | 3.64 % |
| CANN | 1.78 % | 3.66 % |

**Negative binomial specification**

| Model | Mean Squared error | Logarithmic score | 
|:-------------------------------------|:------:|:-----------------:|
| Naïve model (no heterogeneity) | 0.0674 | 0.2437 |
| GLM (no telematics) | 1.47 % | 3.12 % |
| GLM (with handcrafted telematics features) | 1.72 % | 3.54 % |
| CANN | 1.80 % | 3.57 % |
]

**% = improvement over the naïve model**

---
class: inverse, middle

# Thank you for your attention!

.taille-25[
- Code for the project:
  - [.blanc[github.com/francisduval/article_3_count]](https://github.com/francisduval/article_3_count)
- Link to the slideshow:
  - [.blanc[telematics-cann.netlify.app]](https://telematics-cann.netlify.app)
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
